from DrissionPage import ChromiumPage, ChromiumOptions
from bs4 import BeautifulSoup
import os
import time
import csv
import random

def main():
    # --- AYARLAR ---
    csv_dosya_adi = "sahibinden_tum_ilanlar.csv"
    current_dir = os.getcwd()
    profile_path = os.path.join(current_dir, "BenimProfilim")
    
    co = ChromiumOptions()
    co.set_user_data_path(path=profile_path)
    
    # Ninja Modu (GÃ¶rÃ¼nmez ama Headless deÄŸil)
    co.set_argument('--window-position=-10000,-10000') 
    co.set_argument('--blink-settings=imagesEnabled=false')

    # --- CSV HAZIRLIÄI ---
    # DosyayÄ± baÅŸtan oluÅŸtur ve baÅŸlÄ±klarÄ± yaz
    with open(csv_dosya_adi, "w", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        writer.writerow(["Ä°lan No", "BaÅŸlÄ±k", "Fiyat", "Sayfa"]) # Sayfa bilgisini de ekleyelim

    print("ğŸ¥· Ninja TarayÄ±cÄ± BaÅŸlatÄ±lÄ±yor...")
    page = ChromiumPage(co)

    try:
        # 0'dan 980'e kadar, 20'ÅŸer artarak (Toplam 50 Sayfa)
        for offset in range(0, 1000, 20):
            sayfa_no = (offset // 20) + 1
            print(f"\nğŸ”„ Sayfa {sayfa_no} TaranÄ±yor (Offset: {offset})...")
            
            # URL OluÅŸturma
            if offset == 0:
                url = "https://www.sahibinden.com/satilik-daire/istanbul"
            else:
                url = f"https://www.sahibinden.com/satilik-daire/istanbul?pagingOffset={offset}"
            
            # Sayfaya Git
            page.get(url)

            # Tablo KontrolÃ¼
            if page.wait.ele_displayed("#searchResultsTable", timeout=20):
                # HTML'i al ve iÅŸle
                soup = BeautifulSoup(page.html, "html.parser")
                satirlar = soup.find_all("tr", attrs={"data-id": True})
                
                print(f"   âœ… {len(satirlar)} ilan bulundu. Kaydediliyor...")

                # Bu sayfanÄ±n verilerini geÃ§ici listeye al
                sayfa_verileri = []
                for satir in satirlar:
                    try:
                        baslik_tag = satir.find("a", class_="classifiedTitle")
                        baslik = baslik_tag.text.strip() if baslik_tag else "BaÅŸlÄ±k Yok"

                        fiyat_tag = satir.find("td", class_="searchResultsPriceValue")
                        fiyat = fiyat_tag.text.strip() if fiyat_tag else "Fiyat Yok"

                        ilan_no = satir["data-id"]
                        
                        sayfa_verileri.append([ilan_no, baslik, fiyat, sayfa_no])
                    except:
                        continue
                
                # --- ANLIK KAYIT (APPEND MODE) ---
                # Her sayfadan sonra dosyayÄ± aÃ§Ä±p ekleyip kapatÄ±yoruz.
                # BÃ¶ylece kod patlasa bile Ã§ekilenler elimizde kalÄ±r.
                with open(csv_dosya_adi, "a", newline="", encoding="utf-8") as f:
                    writer = csv.writer(f)
                    writer.writerows(sayfa_verileri)

                # Ä°nsan Taklidi (Bekleme SÃ¼resi)
                # 50 sayfa gezeceÄŸimiz iÃ§in dikkat Ã§ekmemek lazÄ±m.
                # 3 ile 6 saniye arasÄ± rastgele bekle.
                bekleme = random.uniform(3, 6)
                print(f"   ğŸ’¾ Kaydedildi. {bekleme:.2f} saniye dinleniyor...")
                time.sleep(bekleme)

            else:
                print("   âŒ Sayfa yÃ¼klenemedi veya bot korumasÄ±na takÄ±ldÄ±k!")
                # EÄŸer korumaya takÄ±lÄ±rsak dÃ¶ngÃ¼yÃ¼ kÄ±rmalÄ±yÄ±z ki boÅŸuna dÃ¶nmesin
                break

    except Exception as e:
        print(f"ğŸ’¥ Genel Hata: {e}")
        
    finally:
        print(f"\nğŸ Ä°ÅŸlem bitti. Veriler '{csv_dosya_adi}' dosyasÄ±na iÅŸlendi.")
        page.quit()

if __name__ == "__main__":
    main()